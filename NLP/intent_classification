{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intent_classification","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jgbjBG4VENr9","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.lancaster import LancasterStemmer\n","import nltk\n","import re\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n","from keras.callbacks import ModelCheckpoint"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Idr4YVaDKZLc","colab_type":"code","colab":{}},"source":["def load_dataset(filename):\n","  df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n","  print(df.head())\n","  intent = df[\"Intent\"]\n","  unique_intent = list(set(intent))\n","  sentences = list(df[\"Sentence\"])\n","  \n","  return (intent, unique_intent, sentences)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pMgRsQmP0EB","colab_type":"code","outputId":"cbe25ef6-b48b-487c-fe1b-79a6aa8cfa7f","executionInfo":{"status":"ok","timestamp":1567494468717,"user_tz":-330,"elapsed":1209,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["intent, unique_intent, sentences = load_dataset(\"/content/intent_data.csv\")"],"execution_count":60,"outputs":[{"output_type":"stream","text":["                Sentence          Intent\n","0       Need help pleese  commonQ.assist\n","1              Need help  commonQ.assist\n","2       I need some info  commonQ.assist\n","3      Will you help me?  commonQ.assist\n","4  What else can you do?  commonQ.assist\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UmI_iCwzP0P6","colab_type":"code","outputId":"325fa7c5-a1a9-4df6-a342-b08c97269957","executionInfo":{"status":"ok","timestamp":1567494460131,"user_tz":-330,"elapsed":1185,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df=pd.read_csv('/content/intent_data.csv',encoding='latin1')\n","print(len(df))"],"execution_count":59,"outputs":[{"output_type":"stream","text":["1112\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t5mAlAHJP0gq","colab_type":"code","outputId":"aadcd25d-f49b-4467-d522-9215bdeaf49c","executionInfo":{"status":"ok","timestamp":1567484483627,"user_tz":-330,"elapsed":394,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"JP1yMhGDP0o0","colab_type":"code","colab":{}},"source":["#define stemmer\n","stemmer = LancasterStemmer()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9c7pzpxP0xI","colab_type":"code","colab":{}},"source":["def cleaning(sentences):\n","  words = []\n","  for s in sentences:\n","    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n","    w = word_tokenize(clean)\n","    #stemming\n","    words.append([i.lower() for i in w])\n","    \n","  return words  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XakM9gWEQ3kC","colab_type":"code","outputId":"f0d131e8-bb49-465f-c7e3-15ade7eb6090","executionInfo":{"status":"ok","timestamp":1567484483629,"user_tz":-330,"elapsed":362,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["cleaned_words = cleaning(sentences)\n","print(len(cleaned_words))\n","print(cleaned_words[:2])  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["1113\n","[['need', 'help', 'pleese'], ['need', 'help']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_BPl3GAgQ3r0","colab_type":"code","colab":{}},"source":["def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n","  token = Tokenizer(filters = filters)\n","  token.fit_on_texts(words)\n","  return token"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KJ86MVsQ31D","colab_type":"code","colab":{}},"source":["def max_length(words):\n","  return(len(max(words, key = len)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_OMX-RLRBzS","colab_type":"code","outputId":"cdda1d6e-f19c-4583-ecfc-98fcd773b93b","executionInfo":{"status":"ok","timestamp":1567484483632,"user_tz":-330,"elapsed":331,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word_tokenizer = create_tokenizer(cleaned_words)\n","vocab_size = len(word_tokenizer.word_index) + 1\n","max_length = max_length(cleaned_words)\n","\n","print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocab Size = 492 and Maximum length = 28\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8vP-02qORB5N","colab_type":"code","colab":{}},"source":["def encoding_doc(token, words):\n","  return(token.texts_to_sequences(words))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6EimqCzzRB-o","colab_type":"code","colab":{}},"source":["encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DwBxg8RtRI1V","colab_type":"code","colab":{}},"source":["def padding_doc(encoded_doc, max_length):\n","  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gIkIHwJRKHe","colab_type":"code","colab":{}},"source":["padded_doc = padding_doc(encoded_doc, max_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UrC39OiRKU6","colab_type":"code","outputId":"c072e535-3144-490f-de9e-f0b7eaad9f84","executionInfo":{"status":"ok","timestamp":1567484483636,"user_tz":-330,"elapsed":277,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["padded_doc[:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 25,  77, 332,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [ 25,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  1,  25, 198, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [ 51,  10,  77,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  8, 268,   4,  10,  30,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"3gGxb-7HRKa_","colab_type":"code","outputId":"8b53bbfa-9724-4e9b-ee34-d372c954e19b","executionInfo":{"status":"ok","timestamp":1567484483636,"user_tz":-330,"elapsed":266,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"Shape of padded docs = \",padded_doc.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of padded docs =  (1113, 28)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qckaKvvbRKqO","colab_type":"code","colab":{}},"source":["#tokenizer with filter changed\n","output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYjkThYsRKyL","colab_type":"code","outputId":"f3990a8d-f401-403c-899a-20e431aec73b","executionInfo":{"status":"ok","timestamp":1567484483637,"user_tz":-330,"elapsed":244,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["output_tokenizer.word_index"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'commonq.assist': 14,\n"," 'commonq.bot': 12,\n"," 'commonq.how': 8,\n"," 'commonq.just_details': 9,\n"," 'commonq.name': 16,\n"," 'commonq.not_giving': 4,\n"," 'commonq.query': 6,\n"," 'commonq.wait': 3,\n"," 'contact.contact': 7,\n"," 'faq.aadhaar_missing': 5,\n"," 'faq.address_proof': 1,\n"," 'faq.application_process': 2,\n"," 'faq.apply_register': 19,\n"," 'faq.approval_time': 10,\n"," 'faq.bad_service': 21,\n"," 'faq.banking_option_missing': 11,\n"," 'faq.biz_category_missing': 17,\n"," 'faq.biz_new': 18,\n"," 'faq.biz_simpler': 20,\n"," 'faq.borrow_limit': 13,\n"," 'faq.borrow_use': 15}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"dyPYQY5TRWqg","colab_type":"code","colab":{}},"source":["encoded_output = encoding_doc(output_tokenizer, intent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mU9jEejqRWvK","colab_type":"code","colab":{}},"source":["encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Py71jYyeRWz5","colab_type":"code","outputId":"32285539-6a98-4579-98ca-78465f48cb76","executionInfo":{"status":"ok","timestamp":1567484483639,"user_tz":-330,"elapsed":212,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoded_output.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1113, 1)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"6B08njpMRW4p","colab_type":"code","colab":{}},"source":["def one_hot(encode):\n","  o = OneHotEncoder(sparse = False)\n","  return(o.fit_transform(encode))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgzZzs_PRW9m","colab_type":"code","outputId":"4ac15cdc-1bb7-40e9-8c3d-2e61ae22ef96","executionInfo":{"status":"ok","timestamp":1567484483640,"user_tz":-330,"elapsed":189,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["output_one_hot = one_hot(encoded_output)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n","If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n","In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3g0Mm9rdRgOr","colab_type":"code","outputId":"f93d446b-31d1-4bab-d663-89fe71d7b609","executionInfo":{"status":"ok","timestamp":1567484483640,"user_tz":-330,"elapsed":179,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["output_one_hot.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1113, 21)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"mUGuV-y8RgUN","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NqOfNmLFRgYr","colab_type":"code","colab":{}},"source":["train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBBIyo-TRmsG","colab_type":"code","outputId":"1781ad23-0daf-46c2-a4eb-ab7eecef4b6c","executionInfo":{"status":"ok","timestamp":1567484483642,"user_tz":-330,"elapsed":142,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n","print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of train_X = (890, 28) and train_Y = (890, 21)\n","Shape of val_X = (223, 28) and val_Y = (223, 21)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tx6lliR8RmzV","colab_type":"code","colab":{}},"source":["def create_model(vocab_size, max_length):\n","  model = Sequential()\n","  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n","  model.add(Bidirectional(LSTM(128)))\n","#   model.add(LSTM(128))\n","  model.add(Dense(32, activation = \"relu\"))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(21, activation = \"softmax\"))\n","  \n","  return model\n","\n","# def create_model(vocab_size, max_length):\n","# \t# create model\n","# \tmodel = Sequential()\n","#   model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n","#   model.add(Bidirectional(LSTM(128)))\n","# \tmodel.add(Dropout(0.2, input_shape=(60,)))\n","# \tmodel.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n","# \tmodel.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n","# \tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n","# \t# Compile model\n","# \tsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n","# \tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","# \treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-nJCGmDRm4m","colab_type":"code","outputId":"2b64a836-318f-4993-bbb6-f12d3c41656e","executionInfo":{"status":"ok","timestamp":1567484483643,"user_tz":-330,"elapsed":116,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["model = create_model(vocab_size, max_length)\n","\n","model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0903 04:20:25.069121 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0903 04:20:25.110664 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0903 04:20:25.118545 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0903 04:20:25.754643 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0903 04:20:25.764465 140330623145856 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0903 04:20:25.793615 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0903 04:20:25.818774 140330623145856 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 28, 128)           62976     \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 256)               263168    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                8224      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 32)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 21)                693       \n","=================================================================\n","Total params: 335,061\n","Trainable params: 272,085\n","Non-trainable params: 62,976\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BoJ-8jSoRm9M","colab_type":"code","outputId":"d1a3a46f-1441-4862-948d-1da6b4f39c1c","executionInfo":{"status":"ok","timestamp":1567491140743,"user_tz":-330,"elapsed":1438678,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","hist = model.fit(train_X, train_Y, epochs = 400, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 890 samples, validate on 223 samples\n","Epoch 1/400\n","890/890 [==============================] - 5s 6ms/step - loss: 0.8588 - acc: 0.7326 - val_loss: 1.0286 - val_acc: 0.6996\n","\n","Epoch 00001: val_loss improved from inf to 1.02858, saving model to model.h5\n","Epoch 2/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.9239 - acc: 0.7213 - val_loss: 1.0059 - val_acc: 0.6996\n","\n","Epoch 00002: val_loss improved from 1.02858 to 1.00586, saving model to model.h5\n","Epoch 3/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8726 - acc: 0.7281 - val_loss: 1.0577 - val_acc: 0.6996\n","\n","Epoch 00003: val_loss did not improve from 1.00586\n","Epoch 4/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8888 - acc: 0.7281 - val_loss: 1.0891 - val_acc: 0.6951\n","\n","Epoch 00004: val_loss did not improve from 1.00586\n","Epoch 5/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8321 - acc: 0.7348 - val_loss: 1.0296 - val_acc: 0.7085\n","\n","Epoch 00005: val_loss did not improve from 1.00586\n","Epoch 6/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7868 - acc: 0.7528 - val_loss: 1.2099 - val_acc: 0.6996\n","\n","Epoch 00006: val_loss did not improve from 1.00586\n","Epoch 7/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8978 - acc: 0.7202 - val_loss: 1.0939 - val_acc: 0.7085\n","\n","Epoch 00007: val_loss did not improve from 1.00586\n","Epoch 8/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8142 - acc: 0.7449 - val_loss: 1.0237 - val_acc: 0.7265\n","\n","Epoch 00008: val_loss did not improve from 1.00586\n","Epoch 9/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7699 - acc: 0.7461 - val_loss: 1.1114 - val_acc: 0.7444\n","\n","Epoch 00009: val_loss did not improve from 1.00586\n","Epoch 10/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7486 - acc: 0.7708 - val_loss: 1.0989 - val_acc: 0.7354\n","\n","Epoch 00010: val_loss did not improve from 1.00586\n","Epoch 11/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7755 - acc: 0.7596 - val_loss: 1.0980 - val_acc: 0.7309\n","\n","Epoch 00011: val_loss did not improve from 1.00586\n","Epoch 12/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7573 - acc: 0.7663 - val_loss: 0.9644 - val_acc: 0.7309\n","\n","Epoch 00012: val_loss improved from 1.00586 to 0.96436, saving model to model.h5\n","Epoch 13/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7093 - acc: 0.7719 - val_loss: 1.0315 - val_acc: 0.7220\n","\n","Epoch 00013: val_loss did not improve from 0.96436\n","Epoch 14/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8088 - acc: 0.7348 - val_loss: 1.3697 - val_acc: 0.6906\n","\n","Epoch 00014: val_loss did not improve from 0.96436\n","Epoch 15/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.8488 - acc: 0.7337 - val_loss: 1.0642 - val_acc: 0.7309\n","\n","Epoch 00015: val_loss did not improve from 0.96436\n","Epoch 16/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7355 - acc: 0.7494 - val_loss: 1.1435 - val_acc: 0.7354\n","\n","Epoch 00016: val_loss did not improve from 0.96436\n","Epoch 17/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6760 - acc: 0.7843 - val_loss: 1.1092 - val_acc: 0.7175\n","\n","Epoch 00017: val_loss did not improve from 0.96436\n","Epoch 18/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6155 - acc: 0.8079 - val_loss: 1.0368 - val_acc: 0.7444\n","\n","Epoch 00018: val_loss did not improve from 0.96436\n","Epoch 19/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6223 - acc: 0.7955 - val_loss: 1.1263 - val_acc: 0.7265\n","\n","Epoch 00019: val_loss did not improve from 0.96436\n","Epoch 20/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6390 - acc: 0.7933 - val_loss: 1.2578 - val_acc: 0.7130\n","\n","Epoch 00020: val_loss did not improve from 0.96436\n","Epoch 21/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6695 - acc: 0.7809 - val_loss: 1.0640 - val_acc: 0.7444\n","\n","Epoch 00021: val_loss did not improve from 0.96436\n","Epoch 22/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7400 - acc: 0.7753 - val_loss: 1.1065 - val_acc: 0.7354\n","\n","Epoch 00022: val_loss did not improve from 0.96436\n","Epoch 23/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6640 - acc: 0.7809 - val_loss: 1.0777 - val_acc: 0.7399\n","\n","Epoch 00023: val_loss did not improve from 0.96436\n","Epoch 24/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6076 - acc: 0.7944 - val_loss: 0.9916 - val_acc: 0.7578\n","\n","Epoch 00024: val_loss did not improve from 0.96436\n","Epoch 25/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5791 - acc: 0.8112 - val_loss: 1.1705 - val_acc: 0.7354\n","\n","Epoch 00025: val_loss did not improve from 0.96436\n","Epoch 26/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5905 - acc: 0.8135 - val_loss: 1.0097 - val_acc: 0.7534\n","\n","Epoch 00026: val_loss did not improve from 0.96436\n","Epoch 27/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5575 - acc: 0.8124 - val_loss: 1.0139 - val_acc: 0.7758\n","\n","Epoch 00027: val_loss did not improve from 0.96436\n","Epoch 28/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5746 - acc: 0.8169 - val_loss: 1.1082 - val_acc: 0.7489\n","\n","Epoch 00028: val_loss did not improve from 0.96436\n","Epoch 29/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6048 - acc: 0.8034 - val_loss: 1.1004 - val_acc: 0.7399\n","\n","Epoch 00029: val_loss did not improve from 0.96436\n","Epoch 30/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5600 - acc: 0.8202 - val_loss: 1.1401 - val_acc: 0.7399\n","\n","Epoch 00030: val_loss did not improve from 0.96436\n","Epoch 31/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5446 - acc: 0.8202 - val_loss: 1.0722 - val_acc: 0.7623\n","\n","Epoch 00031: val_loss did not improve from 0.96436\n","Epoch 32/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5122 - acc: 0.8236 - val_loss: 1.0464 - val_acc: 0.7534\n","\n","Epoch 00032: val_loss did not improve from 0.96436\n","Epoch 33/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5218 - acc: 0.8281 - val_loss: 1.2752 - val_acc: 0.7444\n","\n","Epoch 00033: val_loss did not improve from 0.96436\n","Epoch 34/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5823 - acc: 0.8135 - val_loss: 0.9420 - val_acc: 0.7803\n","\n","Epoch 00034: val_loss improved from 0.96436 to 0.94199, saving model to model.h5\n","Epoch 35/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5577 - acc: 0.8202 - val_loss: 1.0611 - val_acc: 0.7534\n","\n","Epoch 00035: val_loss did not improve from 0.94199\n","Epoch 36/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5258 - acc: 0.8337 - val_loss: 0.9783 - val_acc: 0.7803\n","\n","Epoch 00036: val_loss did not improve from 0.94199\n","Epoch 37/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4896 - acc: 0.8315 - val_loss: 0.9934 - val_acc: 0.7892\n","\n","Epoch 00037: val_loss did not improve from 0.94199\n","Epoch 38/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4976 - acc: 0.8427 - val_loss: 1.0796 - val_acc: 0.7758\n","\n","Epoch 00038: val_loss did not improve from 0.94199\n","Epoch 39/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5291 - acc: 0.8258 - val_loss: 1.1579 - val_acc: 0.7623\n","\n","Epoch 00039: val_loss did not improve from 0.94199\n","Epoch 40/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6627 - acc: 0.8090 - val_loss: 1.1172 - val_acc: 0.7175\n","\n","Epoch 00040: val_loss did not improve from 0.94199\n","Epoch 41/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6022 - acc: 0.8000 - val_loss: 1.1147 - val_acc: 0.7489\n","\n","Epoch 00041: val_loss did not improve from 0.94199\n","Epoch 42/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5689 - acc: 0.8225 - val_loss: 0.9956 - val_acc: 0.7713\n","\n","Epoch 00042: val_loss did not improve from 0.94199\n","Epoch 43/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5154 - acc: 0.8292 - val_loss: 1.0183 - val_acc: 0.7578\n","\n","Epoch 00043: val_loss did not improve from 0.94199\n","Epoch 44/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6117 - acc: 0.8022 - val_loss: 1.0528 - val_acc: 0.7713\n","\n","Epoch 00044: val_loss did not improve from 0.94199\n","Epoch 45/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4806 - acc: 0.8539 - val_loss: 0.9868 - val_acc: 0.7668\n","\n","Epoch 00045: val_loss did not improve from 0.94199\n","Epoch 46/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4793 - acc: 0.8382 - val_loss: 0.9809 - val_acc: 0.7758\n","\n","Epoch 00046: val_loss did not improve from 0.94199\n","Epoch 47/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4624 - acc: 0.8427 - val_loss: 1.0016 - val_acc: 0.7848\n","\n","Epoch 00047: val_loss did not improve from 0.94199\n","Epoch 48/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4909 - acc: 0.8449 - val_loss: 1.0102 - val_acc: 0.7848\n","\n","Epoch 00048: val_loss did not improve from 0.94199\n","Epoch 49/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5128 - acc: 0.8393 - val_loss: 1.0010 - val_acc: 0.7668\n","\n","Epoch 00049: val_loss did not improve from 0.94199\n","Epoch 50/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4772 - acc: 0.8438 - val_loss: 1.1035 - val_acc: 0.7623\n","\n","Epoch 00050: val_loss did not improve from 0.94199\n","Epoch 51/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5370 - acc: 0.8180 - val_loss: 0.8851 - val_acc: 0.7892\n","\n","Epoch 00051: val_loss improved from 0.94199 to 0.88509, saving model to model.h5\n","Epoch 52/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4813 - acc: 0.8449 - val_loss: 0.9567 - val_acc: 0.7713\n","\n","Epoch 00052: val_loss did not improve from 0.88509\n","Epoch 53/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4268 - acc: 0.8607 - val_loss: 1.0321 - val_acc: 0.7758\n","\n","Epoch 00053: val_loss did not improve from 0.88509\n","Epoch 54/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4320 - acc: 0.8562 - val_loss: 0.9713 - val_acc: 0.7758\n","\n","Epoch 00054: val_loss did not improve from 0.88509\n","Epoch 55/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4173 - acc: 0.8629 - val_loss: 1.0174 - val_acc: 0.7937\n","\n","Epoch 00055: val_loss did not improve from 0.88509\n","Epoch 56/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3705 - acc: 0.8719 - val_loss: 1.0325 - val_acc: 0.7803\n","\n","Epoch 00056: val_loss did not improve from 0.88509\n","Epoch 57/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4137 - acc: 0.8539 - val_loss: 1.0500 - val_acc: 0.7803\n","\n","Epoch 00057: val_loss did not improve from 0.88509\n","Epoch 58/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3930 - acc: 0.8708 - val_loss: 1.0424 - val_acc: 0.7848\n","\n","Epoch 00058: val_loss did not improve from 0.88509\n","Epoch 59/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3765 - acc: 0.8730 - val_loss: 1.0071 - val_acc: 0.7848\n","\n","Epoch 00059: val_loss did not improve from 0.88509\n","Epoch 60/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3963 - acc: 0.8652 - val_loss: 1.1211 - val_acc: 0.7982\n","\n","Epoch 00060: val_loss did not improve from 0.88509\n","Epoch 61/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3978 - acc: 0.8708 - val_loss: 1.1462 - val_acc: 0.7758\n","\n","Epoch 00061: val_loss did not improve from 0.88509\n","Epoch 62/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3719 - acc: 0.8775 - val_loss: 1.0740 - val_acc: 0.7848\n","\n","Epoch 00062: val_loss did not improve from 0.88509\n","Epoch 63/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7730 - acc: 0.8022 - val_loss: 1.0389 - val_acc: 0.7623\n","\n","Epoch 00063: val_loss did not improve from 0.88509\n","Epoch 64/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5619 - acc: 0.8382 - val_loss: 0.8800 - val_acc: 0.7758\n","\n","Epoch 00064: val_loss improved from 0.88509 to 0.88000, saving model to model.h5\n","Epoch 65/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4548 - acc: 0.8483 - val_loss: 0.8584 - val_acc: 0.7982\n","\n","Epoch 00065: val_loss improved from 0.88000 to 0.85838, saving model to model.h5\n","Epoch 66/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4151 - acc: 0.8730 - val_loss: 0.8758 - val_acc: 0.7803\n","\n","Epoch 00066: val_loss did not improve from 0.85838\n","Epoch 67/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4035 - acc: 0.8584 - val_loss: 0.9498 - val_acc: 0.7937\n","\n","Epoch 00067: val_loss did not improve from 0.85838\n","Epoch 68/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4864 - acc: 0.8584 - val_loss: 0.9516 - val_acc: 0.7937\n","\n","Epoch 00068: val_loss did not improve from 0.85838\n","Epoch 69/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3997 - acc: 0.8742 - val_loss: 0.9216 - val_acc: 0.7848\n","\n","Epoch 00069: val_loss did not improve from 0.85838\n","Epoch 70/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4004 - acc: 0.8663 - val_loss: 0.8913 - val_acc: 0.7982\n","\n","Epoch 00070: val_loss did not improve from 0.85838\n","Epoch 71/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3726 - acc: 0.8685 - val_loss: 0.9619 - val_acc: 0.7937\n","\n","Epoch 00071: val_loss did not improve from 0.85838\n","Epoch 72/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3629 - acc: 0.8854 - val_loss: 0.9390 - val_acc: 0.8027\n","\n","Epoch 00072: val_loss did not improve from 0.85838\n","Epoch 73/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3644 - acc: 0.8876 - val_loss: 0.9839 - val_acc: 0.7892\n","\n","Epoch 00073: val_loss did not improve from 0.85838\n","Epoch 74/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3477 - acc: 0.8798 - val_loss: 0.9852 - val_acc: 0.7937\n","\n","Epoch 00074: val_loss did not improve from 0.85838\n","Epoch 75/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3634 - acc: 0.8809 - val_loss: 0.9608 - val_acc: 0.8117\n","\n","Epoch 00075: val_loss did not improve from 0.85838\n","Epoch 76/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3458 - acc: 0.8820 - val_loss: 0.9994 - val_acc: 0.7892\n","\n","Epoch 00076: val_loss did not improve from 0.85838\n","Epoch 77/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3375 - acc: 0.8888 - val_loss: 0.9892 - val_acc: 0.7892\n","\n","Epoch 00077: val_loss did not improve from 0.85838\n","Epoch 78/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3367 - acc: 0.8888 - val_loss: 0.9774 - val_acc: 0.7982\n","\n","Epoch 00078: val_loss did not improve from 0.85838\n","Epoch 79/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3048 - acc: 0.9034 - val_loss: 0.9385 - val_acc: 0.7982\n","\n","Epoch 00079: val_loss did not improve from 0.85838\n","Epoch 80/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3395 - acc: 0.8831 - val_loss: 0.9495 - val_acc: 0.7982\n","\n","Epoch 00080: val_loss did not improve from 0.85838\n","Epoch 81/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3340 - acc: 0.8989 - val_loss: 1.0320 - val_acc: 0.7892\n","\n","Epoch 00081: val_loss did not improve from 0.85838\n","Epoch 82/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3284 - acc: 0.8921 - val_loss: 1.0220 - val_acc: 0.7982\n","\n","Epoch 00082: val_loss did not improve from 0.85838\n","Epoch 83/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3246 - acc: 0.8955 - val_loss: 1.0630 - val_acc: 0.7937\n","\n","Epoch 00083: val_loss did not improve from 0.85838\n","Epoch 84/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3142 - acc: 0.8933 - val_loss: 1.0684 - val_acc: 0.7937\n","\n","Epoch 00084: val_loss did not improve from 0.85838\n","Epoch 85/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3222 - acc: 0.8921 - val_loss: 0.9750 - val_acc: 0.7892\n","\n","Epoch 00085: val_loss did not improve from 0.85838\n","Epoch 86/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2849 - acc: 0.9124 - val_loss: 1.0165 - val_acc: 0.8027\n","\n","Epoch 00086: val_loss did not improve from 0.85838\n","Epoch 87/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2714 - acc: 0.9079 - val_loss: 1.0624 - val_acc: 0.8027\n","\n","Epoch 00087: val_loss did not improve from 0.85838\n","Epoch 88/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2845 - acc: 0.9045 - val_loss: 1.0902 - val_acc: 0.8072\n","\n","Epoch 00088: val_loss did not improve from 0.85838\n","Epoch 89/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3004 - acc: 0.8966 - val_loss: 1.1075 - val_acc: 0.7982\n","\n","Epoch 00089: val_loss did not improve from 0.85838\n","Epoch 90/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2902 - acc: 0.9022 - val_loss: 1.0584 - val_acc: 0.7982\n","\n","Epoch 00090: val_loss did not improve from 0.85838\n","Epoch 91/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3036 - acc: 0.9079 - val_loss: 1.0319 - val_acc: 0.8027\n","\n","Epoch 00091: val_loss did not improve from 0.85838\n","Epoch 92/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3496 - acc: 0.8921 - val_loss: 1.0970 - val_acc: 0.7848\n","\n","Epoch 00092: val_loss did not improve from 0.85838\n","Epoch 93/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3009 - acc: 0.8944 - val_loss: 1.0927 - val_acc: 0.8072\n","\n","Epoch 00093: val_loss did not improve from 0.85838\n","Epoch 94/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2712 - acc: 0.9157 - val_loss: 1.0280 - val_acc: 0.7937\n","\n","Epoch 00094: val_loss did not improve from 0.85838\n","Epoch 95/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2635 - acc: 0.9135 - val_loss: 1.1037 - val_acc: 0.8072\n","\n","Epoch 00095: val_loss did not improve from 0.85838\n","Epoch 96/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3025 - acc: 0.9022 - val_loss: 0.9861 - val_acc: 0.8161\n","\n","Epoch 00096: val_loss did not improve from 0.85838\n","Epoch 97/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3574 - acc: 0.8854 - val_loss: 1.2429 - val_acc: 0.7803\n","\n","Epoch 00097: val_loss did not improve from 0.85838\n","Epoch 98/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3250 - acc: 0.8955 - val_loss: 1.0976 - val_acc: 0.7892\n","\n","Epoch 00098: val_loss did not improve from 0.85838\n","Epoch 99/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2959 - acc: 0.8978 - val_loss: 1.3233 - val_acc: 0.7937\n","\n","Epoch 00099: val_loss did not improve from 0.85838\n","Epoch 100/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.6174 - acc: 0.8303 - val_loss: 1.1616 - val_acc: 0.7848\n","\n","Epoch 00100: val_loss did not improve from 0.85838\n","Epoch 101/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4811 - acc: 0.8607 - val_loss: 0.8179 - val_acc: 0.8117\n","\n","Epoch 00101: val_loss improved from 0.85838 to 0.81793, saving model to model.h5\n","Epoch 102/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3653 - acc: 0.8876 - val_loss: 0.9719 - val_acc: 0.8117\n","\n","Epoch 00102: val_loss did not improve from 0.81793\n","Epoch 103/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2898 - acc: 0.9112 - val_loss: 0.9839 - val_acc: 0.8117\n","\n","Epoch 00103: val_loss did not improve from 0.81793\n","Epoch 104/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3456 - acc: 0.9090 - val_loss: 0.9924 - val_acc: 0.7937\n","\n","Epoch 00104: val_loss did not improve from 0.81793\n","Epoch 105/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3073 - acc: 0.9022 - val_loss: 0.8179 - val_acc: 0.8027\n","\n","Epoch 00105: val_loss improved from 0.81793 to 0.81787, saving model to model.h5\n","Epoch 106/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2848 - acc: 0.9202 - val_loss: 0.8381 - val_acc: 0.8206\n","\n","Epoch 00106: val_loss did not improve from 0.81787\n","Epoch 107/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2721 - acc: 0.9045 - val_loss: 0.9851 - val_acc: 0.8161\n","\n","Epoch 00107: val_loss did not improve from 0.81787\n","Epoch 108/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2316 - acc: 0.9247 - val_loss: 0.9630 - val_acc: 0.8161\n","\n","Epoch 00108: val_loss did not improve from 0.81787\n","Epoch 109/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2450 - acc: 0.9146 - val_loss: 1.0987 - val_acc: 0.8117\n","\n","Epoch 00109: val_loss did not improve from 0.81787\n","Epoch 110/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2223 - acc: 0.9146 - val_loss: 1.0552 - val_acc: 0.8161\n","\n","Epoch 00110: val_loss did not improve from 0.81787\n","Epoch 111/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2198 - acc: 0.9202 - val_loss: 1.1018 - val_acc: 0.8161\n","\n","Epoch 00111: val_loss did not improve from 0.81787\n","Epoch 112/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2500 - acc: 0.9146 - val_loss: 1.1880 - val_acc: 0.8161\n","\n","Epoch 00112: val_loss did not improve from 0.81787\n","Epoch 113/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2253 - acc: 0.9225 - val_loss: 1.1472 - val_acc: 0.8117\n","\n","Epoch 00113: val_loss did not improve from 0.81787\n","Epoch 114/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2675 - acc: 0.9101 - val_loss: 1.0269 - val_acc: 0.8117\n","\n","Epoch 00114: val_loss did not improve from 0.81787\n","Epoch 115/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2408 - acc: 0.9270 - val_loss: 1.2040 - val_acc: 0.8161\n","\n","Epoch 00115: val_loss did not improve from 0.81787\n","Epoch 116/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2713 - acc: 0.9124 - val_loss: 1.1690 - val_acc: 0.8027\n","\n","Epoch 00116: val_loss did not improve from 0.81787\n","Epoch 117/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2358 - acc: 0.9191 - val_loss: 1.1439 - val_acc: 0.8117\n","\n","Epoch 00117: val_loss did not improve from 0.81787\n","Epoch 118/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2142 - acc: 0.9292 - val_loss: 1.2194 - val_acc: 0.8161\n","\n","Epoch 00118: val_loss did not improve from 0.81787\n","Epoch 119/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.2201 - acc: 0.9225 - val_loss: 1.1738 - val_acc: 0.8117\n","\n","Epoch 00119: val_loss did not improve from 0.81787\n","Epoch 120/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2094 - acc: 0.9270 - val_loss: 1.1290 - val_acc: 0.8161\n","\n","Epoch 00120: val_loss did not improve from 0.81787\n","Epoch 121/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2304 - acc: 0.9236 - val_loss: 1.1324 - val_acc: 0.8027\n","\n","Epoch 00121: val_loss did not improve from 0.81787\n","Epoch 122/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1915 - acc: 0.9326 - val_loss: 1.1699 - val_acc: 0.8161\n","\n","Epoch 00122: val_loss did not improve from 0.81787\n","Epoch 123/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2130 - acc: 0.9315 - val_loss: 1.2286 - val_acc: 0.8206\n","\n","Epoch 00123: val_loss did not improve from 0.81787\n","Epoch 124/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2063 - acc: 0.9202 - val_loss: 1.2059 - val_acc: 0.8117\n","\n","Epoch 00124: val_loss did not improve from 0.81787\n","Epoch 125/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2098 - acc: 0.9258 - val_loss: 1.1731 - val_acc: 0.8117\n","\n","Epoch 00125: val_loss did not improve from 0.81787\n","Epoch 126/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2063 - acc: 0.9303 - val_loss: 1.0720 - val_acc: 0.8206\n","\n","Epoch 00126: val_loss did not improve from 0.81787\n","Epoch 127/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2781 - acc: 0.9146 - val_loss: 1.1071 - val_acc: 0.7982\n","\n","Epoch 00127: val_loss did not improve from 0.81787\n","Epoch 128/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2346 - acc: 0.9326 - val_loss: 1.0094 - val_acc: 0.8386\n","\n","Epoch 00128: val_loss did not improve from 0.81787\n","Epoch 129/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1958 - acc: 0.9303 - val_loss: 1.1681 - val_acc: 0.8206\n","\n","Epoch 00129: val_loss did not improve from 0.81787\n","Epoch 130/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2092 - acc: 0.9281 - val_loss: 1.1210 - val_acc: 0.8206\n","\n","Epoch 00130: val_loss did not improve from 0.81787\n","Epoch 131/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1902 - acc: 0.9382 - val_loss: 1.3040 - val_acc: 0.8161\n","\n","Epoch 00131: val_loss did not improve from 0.81787\n","Epoch 132/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2145 - acc: 0.9270 - val_loss: 1.2419 - val_acc: 0.8161\n","\n","Epoch 00132: val_loss did not improve from 0.81787\n","Epoch 133/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2719 - acc: 0.9247 - val_loss: 1.4179 - val_acc: 0.7713\n","\n","Epoch 00133: val_loss did not improve from 0.81787\n","Epoch 134/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4029 - acc: 0.8708 - val_loss: 1.2413 - val_acc: 0.7937\n","\n","Epoch 00134: val_loss did not improve from 0.81787\n","Epoch 135/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3686 - acc: 0.8910 - val_loss: 0.9428 - val_acc: 0.8117\n","\n","Epoch 00135: val_loss did not improve from 0.81787\n","Epoch 136/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4296 - acc: 0.8933 - val_loss: 0.8364 - val_acc: 0.8027\n","\n","Epoch 00136: val_loss did not improve from 0.81787\n","Epoch 137/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4088 - acc: 0.8944 - val_loss: 1.0708 - val_acc: 0.7937\n","\n","Epoch 00137: val_loss did not improve from 0.81787\n","Epoch 138/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3138 - acc: 0.9000 - val_loss: 0.9891 - val_acc: 0.7892\n","\n","Epoch 00138: val_loss did not improve from 0.81787\n","Epoch 139/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2442 - acc: 0.9236 - val_loss: 0.9919 - val_acc: 0.7937\n","\n","Epoch 00139: val_loss did not improve from 0.81787\n","Epoch 140/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2096 - acc: 0.9337 - val_loss: 0.9720 - val_acc: 0.7982\n","\n","Epoch 00140: val_loss did not improve from 0.81787\n","Epoch 141/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1709 - acc: 0.9382 - val_loss: 1.0953 - val_acc: 0.7982\n","\n","Epoch 00141: val_loss did not improve from 0.81787\n","Epoch 142/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2105 - acc: 0.9303 - val_loss: 1.0525 - val_acc: 0.8027\n","\n","Epoch 00142: val_loss did not improve from 0.81787\n","Epoch 143/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1777 - acc: 0.9416 - val_loss: 1.1691 - val_acc: 0.8117\n","\n","Epoch 00143: val_loss did not improve from 0.81787\n","Epoch 144/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2161 - acc: 0.9135 - val_loss: 1.1617 - val_acc: 0.8206\n","\n","Epoch 00144: val_loss did not improve from 0.81787\n","Epoch 145/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1738 - acc: 0.9382 - val_loss: 1.1192 - val_acc: 0.8117\n","\n","Epoch 00145: val_loss did not improve from 0.81787\n","Epoch 146/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1980 - acc: 0.9360 - val_loss: 1.1814 - val_acc: 0.8117\n","\n","Epoch 00146: val_loss did not improve from 0.81787\n","Epoch 147/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1755 - acc: 0.9382 - val_loss: 1.1718 - val_acc: 0.8027\n","\n","Epoch 00147: val_loss did not improve from 0.81787\n","Epoch 148/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1661 - acc: 0.9528 - val_loss: 1.1569 - val_acc: 0.7982\n","\n","Epoch 00148: val_loss did not improve from 0.81787\n","Epoch 149/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1684 - acc: 0.9416 - val_loss: 1.1729 - val_acc: 0.8161\n","\n","Epoch 00149: val_loss did not improve from 0.81787\n","Epoch 150/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2089 - acc: 0.9337 - val_loss: 1.1621 - val_acc: 0.8117\n","\n","Epoch 00150: val_loss did not improve from 0.81787\n","Epoch 151/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2089 - acc: 0.9337 - val_loss: 1.2326 - val_acc: 0.7982\n","\n","Epoch 00151: val_loss did not improve from 0.81787\n","Epoch 152/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1791 - acc: 0.9371 - val_loss: 1.2053 - val_acc: 0.8117\n","\n","Epoch 00152: val_loss did not improve from 0.81787\n","Epoch 153/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1583 - acc: 0.9449 - val_loss: 1.0978 - val_acc: 0.8251\n","\n","Epoch 00153: val_loss did not improve from 0.81787\n","Epoch 154/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1711 - acc: 0.9427 - val_loss: 1.1360 - val_acc: 0.8161\n","\n","Epoch 00154: val_loss did not improve from 0.81787\n","Epoch 155/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1665 - acc: 0.9427 - val_loss: 1.1745 - val_acc: 0.8251\n","\n","Epoch 00155: val_loss did not improve from 0.81787\n","Epoch 156/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1584 - acc: 0.9416 - val_loss: 1.1628 - val_acc: 0.8296\n","\n","Epoch 00156: val_loss did not improve from 0.81787\n","Epoch 157/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1510 - acc: 0.9506 - val_loss: 1.1509 - val_acc: 0.8206\n","\n","Epoch 00157: val_loss did not improve from 0.81787\n","Epoch 158/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1621 - acc: 0.9472 - val_loss: 1.1552 - val_acc: 0.8386\n","\n","Epoch 00158: val_loss did not improve from 0.81787\n","Epoch 159/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1757 - acc: 0.9360 - val_loss: 1.2770 - val_acc: 0.7848\n","\n","Epoch 00159: val_loss did not improve from 0.81787\n","Epoch 160/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2538 - acc: 0.9315 - val_loss: 1.1413 - val_acc: 0.8027\n","\n","Epoch 00160: val_loss did not improve from 0.81787\n","Epoch 161/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2255 - acc: 0.9281 - val_loss: 1.1602 - val_acc: 0.8027\n","\n","Epoch 00161: val_loss did not improve from 0.81787\n","Epoch 162/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1949 - acc: 0.9382 - val_loss: 1.0217 - val_acc: 0.8206\n","\n","Epoch 00162: val_loss did not improve from 0.81787\n","Epoch 163/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1427 - acc: 0.9506 - val_loss: 1.1693 - val_acc: 0.8072\n","\n","Epoch 00163: val_loss did not improve from 0.81787\n","Epoch 164/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1588 - acc: 0.9517 - val_loss: 1.1260 - val_acc: 0.8161\n","\n","Epoch 00164: val_loss did not improve from 0.81787\n","Epoch 165/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1582 - acc: 0.9506 - val_loss: 1.1799 - val_acc: 0.8206\n","\n","Epoch 00165: val_loss did not improve from 0.81787\n","Epoch 166/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1291 - acc: 0.9562 - val_loss: 1.1614 - val_acc: 0.8251\n","\n","Epoch 00166: val_loss did not improve from 0.81787\n","Epoch 167/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1684 - acc: 0.9404 - val_loss: 1.2427 - val_acc: 0.8161\n","\n","Epoch 00167: val_loss did not improve from 0.81787\n","Epoch 168/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1813 - acc: 0.9449 - val_loss: 1.1799 - val_acc: 0.8206\n","\n","Epoch 00168: val_loss did not improve from 0.81787\n","Epoch 169/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1498 - acc: 0.9517 - val_loss: 1.1817 - val_acc: 0.8161\n","\n","Epoch 00169: val_loss did not improve from 0.81787\n","Epoch 170/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1634 - acc: 0.9427 - val_loss: 1.1766 - val_acc: 0.8251\n","\n","Epoch 00170: val_loss did not improve from 0.81787\n","Epoch 171/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1722 - acc: 0.9360 - val_loss: 1.1462 - val_acc: 0.8161\n","\n","Epoch 00171: val_loss did not improve from 0.81787\n","Epoch 172/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1629 - acc: 0.9461 - val_loss: 1.2210 - val_acc: 0.8206\n","\n","Epoch 00172: val_loss did not improve from 0.81787\n","Epoch 173/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1660 - acc: 0.9337 - val_loss: 1.3368 - val_acc: 0.8072\n","\n","Epoch 00173: val_loss did not improve from 0.81787\n","Epoch 174/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2057 - acc: 0.9416 - val_loss: 0.9995 - val_acc: 0.8161\n","\n","Epoch 00174: val_loss did not improve from 0.81787\n","Epoch 175/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2548 - acc: 0.9348 - val_loss: 1.1314 - val_acc: 0.8072\n","\n","Epoch 00175: val_loss did not improve from 0.81787\n","Epoch 176/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2121 - acc: 0.9270 - val_loss: 0.9995 - val_acc: 0.8161\n","\n","Epoch 00176: val_loss did not improve from 0.81787\n","Epoch 177/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1640 - acc: 0.9483 - val_loss: 1.0547 - val_acc: 0.8206\n","\n","Epoch 00177: val_loss did not improve from 0.81787\n","Epoch 178/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1356 - acc: 0.9596 - val_loss: 1.1098 - val_acc: 0.8117\n","\n","Epoch 00178: val_loss did not improve from 0.81787\n","Epoch 179/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1376 - acc: 0.9472 - val_loss: 1.0793 - val_acc: 0.8341\n","\n","Epoch 00179: val_loss did not improve from 0.81787\n","Epoch 180/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1685 - acc: 0.9404 - val_loss: 1.1496 - val_acc: 0.8206\n","\n","Epoch 00180: val_loss did not improve from 0.81787\n","Epoch 181/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1332 - acc: 0.9562 - val_loss: 1.1849 - val_acc: 0.8251\n","\n","Epoch 00181: val_loss did not improve from 0.81787\n","Epoch 182/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1098 - acc: 0.9640 - val_loss: 1.1520 - val_acc: 0.8296\n","\n","Epoch 00182: val_loss did not improve from 0.81787\n","Epoch 183/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1415 - acc: 0.9528 - val_loss: 1.1299 - val_acc: 0.8161\n","\n","Epoch 00183: val_loss did not improve from 0.81787\n","Epoch 184/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1485 - acc: 0.9483 - val_loss: 1.1132 - val_acc: 0.8251\n","\n","Epoch 00184: val_loss did not improve from 0.81787\n","Epoch 185/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1142 - acc: 0.9551 - val_loss: 1.1678 - val_acc: 0.8386\n","\n","Epoch 00185: val_loss did not improve from 0.81787\n","Epoch 186/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1336 - acc: 0.9506 - val_loss: 1.2418 - val_acc: 0.8341\n","\n","Epoch 00186: val_loss did not improve from 0.81787\n","Epoch 187/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1384 - acc: 0.9562 - val_loss: 1.2684 - val_acc: 0.8251\n","\n","Epoch 00187: val_loss did not improve from 0.81787\n","Epoch 188/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1250 - acc: 0.9573 - val_loss: 1.3040 - val_acc: 0.8161\n","\n","Epoch 00188: val_loss did not improve from 0.81787\n","Epoch 189/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1384 - acc: 0.9573 - val_loss: 1.3175 - val_acc: 0.8296\n","\n","Epoch 00189: val_loss did not improve from 0.81787\n","Epoch 190/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1250 - acc: 0.9652 - val_loss: 1.4083 - val_acc: 0.8296\n","\n","Epoch 00190: val_loss did not improve from 0.81787\n","Epoch 191/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1131 - acc: 0.9618 - val_loss: 1.3409 - val_acc: 0.8206\n","\n","Epoch 00191: val_loss did not improve from 0.81787\n","Epoch 192/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1869 - acc: 0.9348 - val_loss: 1.2067 - val_acc: 0.8251\n","\n","Epoch 00192: val_loss did not improve from 0.81787\n","Epoch 193/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1645 - acc: 0.9404 - val_loss: 1.1280 - val_acc: 0.8117\n","\n","Epoch 00193: val_loss did not improve from 0.81787\n","Epoch 194/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2243 - acc: 0.9371 - val_loss: 1.2026 - val_acc: 0.8251\n","\n","Epoch 00194: val_loss did not improve from 0.81787\n","Epoch 195/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2692 - acc: 0.9270 - val_loss: 1.0078 - val_acc: 0.8296\n","\n","Epoch 00195: val_loss did not improve from 0.81787\n","Epoch 196/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1858 - acc: 0.9438 - val_loss: 0.9529 - val_acc: 0.8386\n","\n","Epoch 00196: val_loss did not improve from 0.81787\n","Epoch 197/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1683 - acc: 0.9438 - val_loss: 0.9983 - val_acc: 0.8296\n","\n","Epoch 00197: val_loss did not improve from 0.81787\n","Epoch 198/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1572 - acc: 0.9472 - val_loss: 1.0423 - val_acc: 0.8206\n","\n","Epoch 00198: val_loss did not improve from 0.81787\n","Epoch 199/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1273 - acc: 0.9640 - val_loss: 1.0899 - val_acc: 0.8206\n","\n","Epoch 00199: val_loss did not improve from 0.81787\n","Epoch 200/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1299 - acc: 0.9528 - val_loss: 1.0431 - val_acc: 0.8296\n","\n","Epoch 00200: val_loss did not improve from 0.81787\n","Epoch 201/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1422 - acc: 0.9506 - val_loss: 1.0824 - val_acc: 0.8206\n","\n","Epoch 00201: val_loss did not improve from 0.81787\n","Epoch 202/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1323 - acc: 0.9562 - val_loss: 1.1623 - val_acc: 0.8296\n","\n","Epoch 00202: val_loss did not improve from 0.81787\n","Epoch 203/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1265 - acc: 0.9551 - val_loss: 1.2041 - val_acc: 0.8206\n","\n","Epoch 00203: val_loss did not improve from 0.81787\n","Epoch 204/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1357 - acc: 0.9573 - val_loss: 1.0622 - val_acc: 0.8296\n","\n","Epoch 00204: val_loss did not improve from 0.81787\n","Epoch 205/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1211 - acc: 0.9640 - val_loss: 1.2685 - val_acc: 0.8296\n","\n","Epoch 00205: val_loss did not improve from 0.81787\n","Epoch 206/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1172 - acc: 0.9584 - val_loss: 1.3044 - val_acc: 0.8206\n","\n","Epoch 00206: val_loss did not improve from 0.81787\n","Epoch 207/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1295 - acc: 0.9517 - val_loss: 1.2398 - val_acc: 0.8206\n","\n","Epoch 00207: val_loss did not improve from 0.81787\n","Epoch 208/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1102 - acc: 0.9640 - val_loss: 1.2369 - val_acc: 0.8161\n","\n","Epoch 00208: val_loss did not improve from 0.81787\n","Epoch 209/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2108 - acc: 0.9517 - val_loss: 1.2786 - val_acc: 0.8027\n","\n","Epoch 00209: val_loss did not improve from 0.81787\n","Epoch 210/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3033 - acc: 0.9292 - val_loss: 1.3380 - val_acc: 0.8117\n","\n","Epoch 00210: val_loss did not improve from 0.81787\n","Epoch 211/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1585 - acc: 0.9427 - val_loss: 1.1284 - val_acc: 0.8251\n","\n","Epoch 00211: val_loss did not improve from 0.81787\n","Epoch 212/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1564 - acc: 0.9573 - val_loss: 0.9928 - val_acc: 0.8296\n","\n","Epoch 00212: val_loss did not improve from 0.81787\n","Epoch 213/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1541 - acc: 0.9528 - val_loss: 0.9707 - val_acc: 0.8386\n","\n","Epoch 00213: val_loss did not improve from 0.81787\n","Epoch 214/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1257 - acc: 0.9584 - val_loss: 1.0364 - val_acc: 0.8430\n","\n","Epoch 00214: val_loss did not improve from 0.81787\n","Epoch 215/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1093 - acc: 0.9618 - val_loss: 1.0936 - val_acc: 0.8206\n","\n","Epoch 00215: val_loss did not improve from 0.81787\n","Epoch 216/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1273 - acc: 0.9573 - val_loss: 1.1021 - val_acc: 0.8386\n","\n","Epoch 00216: val_loss did not improve from 0.81787\n","Epoch 217/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1331 - acc: 0.9506 - val_loss: 1.0776 - val_acc: 0.8251\n","\n","Epoch 00217: val_loss did not improve from 0.81787\n","Epoch 218/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1140 - acc: 0.9652 - val_loss: 1.1887 - val_acc: 0.8341\n","\n","Epoch 00218: val_loss did not improve from 0.81787\n","Epoch 219/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1022 - acc: 0.9629 - val_loss: 1.2737 - val_acc: 0.8341\n","\n","Epoch 00219: val_loss did not improve from 0.81787\n","Epoch 220/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1216 - acc: 0.9528 - val_loss: 1.2136 - val_acc: 0.8386\n","\n","Epoch 00220: val_loss did not improve from 0.81787\n","Epoch 221/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1196 - acc: 0.9562 - val_loss: 1.1870 - val_acc: 0.8296\n","\n","Epoch 00221: val_loss did not improve from 0.81787\n","Epoch 222/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1137 - acc: 0.9629 - val_loss: 1.0668 - val_acc: 0.8296\n","\n","Epoch 00222: val_loss did not improve from 0.81787\n","Epoch 223/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1031 - acc: 0.9652 - val_loss: 1.2108 - val_acc: 0.8296\n","\n","Epoch 00223: val_loss did not improve from 0.81787\n","Epoch 224/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0996 - acc: 0.9663 - val_loss: 1.2374 - val_acc: 0.8341\n","\n","Epoch 00224: val_loss did not improve from 0.81787\n","Epoch 225/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1084 - acc: 0.9629 - val_loss: 1.2860 - val_acc: 0.8386\n","\n","Epoch 00225: val_loss did not improve from 0.81787\n","Epoch 226/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0896 - acc: 0.9629 - val_loss: 1.2763 - val_acc: 0.8430\n","\n","Epoch 00226: val_loss did not improve from 0.81787\n","Epoch 227/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0984 - acc: 0.9596 - val_loss: 1.2478 - val_acc: 0.8296\n","\n","Epoch 00227: val_loss did not improve from 0.81787\n","Epoch 228/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0921 - acc: 0.9629 - val_loss: 1.3994 - val_acc: 0.8341\n","\n","Epoch 00228: val_loss did not improve from 0.81787\n","Epoch 229/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1301 - acc: 0.9584 - val_loss: 1.2776 - val_acc: 0.8206\n","\n","Epoch 00229: val_loss did not improve from 0.81787\n","Epoch 230/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1168 - acc: 0.9539 - val_loss: 1.1302 - val_acc: 0.8475\n","\n","Epoch 00230: val_loss did not improve from 0.81787\n","Epoch 231/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1085 - acc: 0.9652 - val_loss: 1.1694 - val_acc: 0.8430\n","\n","Epoch 00231: val_loss did not improve from 0.81787\n","Epoch 232/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0972 - acc: 0.9652 - val_loss: 1.2772 - val_acc: 0.8251\n","\n","Epoch 00232: val_loss did not improve from 0.81787\n","Epoch 233/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1185 - acc: 0.9640 - val_loss: 1.1501 - val_acc: 0.8430\n","\n","Epoch 00233: val_loss did not improve from 0.81787\n","Epoch 234/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1151 - acc: 0.9618 - val_loss: 1.1878 - val_acc: 0.8386\n","\n","Epoch 00234: val_loss did not improve from 0.81787\n","Epoch 235/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0792 - acc: 0.9730 - val_loss: 1.1605 - val_acc: 0.8296\n","\n","Epoch 00235: val_loss did not improve from 0.81787\n","Epoch 236/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0861 - acc: 0.9708 - val_loss: 1.3671 - val_acc: 0.8251\n","\n","Epoch 00236: val_loss did not improve from 0.81787\n","Epoch 237/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1114 - acc: 0.9640 - val_loss: 1.2379 - val_acc: 0.8386\n","\n","Epoch 00237: val_loss did not improve from 0.81787\n","Epoch 238/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1127 - acc: 0.9596 - val_loss: 1.1492 - val_acc: 0.8341\n","\n","Epoch 00238: val_loss did not improve from 0.81787\n","Epoch 239/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0887 - acc: 0.9708 - val_loss: 1.2514 - val_acc: 0.8386\n","\n","Epoch 00239: val_loss did not improve from 0.81787\n","Epoch 240/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1279 - acc: 0.9494 - val_loss: 1.1939 - val_acc: 0.8296\n","\n","Epoch 00240: val_loss did not improve from 0.81787\n","Epoch 241/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1368 - acc: 0.9551 - val_loss: 1.2548 - val_acc: 0.8296\n","\n","Epoch 00241: val_loss did not improve from 0.81787\n","Epoch 242/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.7714 - acc: 0.8584 - val_loss: 1.1544 - val_acc: 0.7848\n","\n","Epoch 00242: val_loss did not improve from 0.81787\n","Epoch 243/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.5272 - acc: 0.8742 - val_loss: 0.8893 - val_acc: 0.8341\n","\n","Epoch 00243: val_loss did not improve from 0.81787\n","Epoch 244/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2163 - acc: 0.9360 - val_loss: 0.8398 - val_acc: 0.8251\n","\n","Epoch 00244: val_loss did not improve from 0.81787\n","Epoch 245/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1672 - acc: 0.9506 - val_loss: 1.0851 - val_acc: 0.8027\n","\n","Epoch 00245: val_loss did not improve from 0.81787\n","Epoch 246/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1818 - acc: 0.9404 - val_loss: 0.9023 - val_acc: 0.8296\n","\n","Epoch 00246: val_loss did not improve from 0.81787\n","Epoch 247/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1493 - acc: 0.9539 - val_loss: 1.0016 - val_acc: 0.8430\n","\n","Epoch 00247: val_loss did not improve from 0.81787\n","Epoch 248/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1379 - acc: 0.9551 - val_loss: 1.0032 - val_acc: 0.8430\n","\n","Epoch 00248: val_loss did not improve from 0.81787\n","Epoch 249/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1241 - acc: 0.9596 - val_loss: 1.0235 - val_acc: 0.8341\n","\n","Epoch 00249: val_loss did not improve from 0.81787\n","Epoch 250/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1095 - acc: 0.9685 - val_loss: 1.0641 - val_acc: 0.8386\n","\n","Epoch 00250: val_loss did not improve from 0.81787\n","Epoch 251/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1087 - acc: 0.9663 - val_loss: 1.0703 - val_acc: 0.8430\n","\n","Epoch 00251: val_loss did not improve from 0.81787\n","Epoch 252/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1245 - acc: 0.9517 - val_loss: 0.9752 - val_acc: 0.8520\n","\n","Epoch 00252: val_loss did not improve from 0.81787\n","Epoch 253/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1066 - acc: 0.9629 - val_loss: 0.9888 - val_acc: 0.8520\n","\n","Epoch 00253: val_loss did not improve from 0.81787\n","Epoch 254/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1166 - acc: 0.9517 - val_loss: 1.0585 - val_acc: 0.8430\n","\n","Epoch 00254: val_loss did not improve from 0.81787\n","Epoch 255/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1788 - acc: 0.9461 - val_loss: 1.1891 - val_acc: 0.8072\n","\n","Epoch 00255: val_loss did not improve from 0.81787\n","Epoch 256/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1759 - acc: 0.9427 - val_loss: 1.0392 - val_acc: 0.8296\n","\n","Epoch 00256: val_loss did not improve from 0.81787\n","Epoch 257/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1339 - acc: 0.9539 - val_loss: 0.8820 - val_acc: 0.8341\n","\n","Epoch 00257: val_loss did not improve from 0.81787\n","Epoch 258/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1153 - acc: 0.9596 - val_loss: 1.0374 - val_acc: 0.8206\n","\n","Epoch 00258: val_loss did not improve from 0.81787\n","Epoch 259/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1144 - acc: 0.9607 - val_loss: 0.9736 - val_acc: 0.8251\n","\n","Epoch 00259: val_loss did not improve from 0.81787\n","Epoch 260/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1154 - acc: 0.9517 - val_loss: 1.0861 - val_acc: 0.8117\n","\n","Epoch 00260: val_loss did not improve from 0.81787\n","Epoch 261/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1019 - acc: 0.9685 - val_loss: 0.9545 - val_acc: 0.8341\n","\n","Epoch 00261: val_loss did not improve from 0.81787\n","Epoch 262/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1009 - acc: 0.9629 - val_loss: 1.0744 - val_acc: 0.8206\n","\n","Epoch 00262: val_loss did not improve from 0.81787\n","Epoch 263/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1204 - acc: 0.9539 - val_loss: 1.0197 - val_acc: 0.8251\n","\n","Epoch 00263: val_loss did not improve from 0.81787\n","Epoch 264/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0979 - acc: 0.9663 - val_loss: 0.9981 - val_acc: 0.8565\n","\n","Epoch 00264: val_loss did not improve from 0.81787\n","Epoch 265/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0899 - acc: 0.9708 - val_loss: 0.9958 - val_acc: 0.8430\n","\n","Epoch 00265: val_loss did not improve from 0.81787\n","Epoch 266/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0944 - acc: 0.9719 - val_loss: 1.0288 - val_acc: 0.8475\n","\n","Epoch 00266: val_loss did not improve from 0.81787\n","Epoch 267/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0820 - acc: 0.9663 - val_loss: 1.2542 - val_acc: 0.8251\n","\n","Epoch 00267: val_loss did not improve from 0.81787\n","Epoch 268/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1130 - acc: 0.9551 - val_loss: 0.9842 - val_acc: 0.8520\n","\n","Epoch 00268: val_loss did not improve from 0.81787\n","Epoch 269/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0716 - acc: 0.9798 - val_loss: 1.0142 - val_acc: 0.8520\n","\n","Epoch 00269: val_loss did not improve from 0.81787\n","Epoch 270/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0859 - acc: 0.9697 - val_loss: 1.0790 - val_acc: 0.8430\n","\n","Epoch 00270: val_loss did not improve from 0.81787\n","Epoch 271/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0944 - acc: 0.9663 - val_loss: 1.1050 - val_acc: 0.8386\n","\n","Epoch 00271: val_loss did not improve from 0.81787\n","Epoch 272/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0959 - acc: 0.9685 - val_loss: 1.2085 - val_acc: 0.8386\n","\n","Epoch 00272: val_loss did not improve from 0.81787\n","Epoch 273/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0887 - acc: 0.9719 - val_loss: 1.0823 - val_acc: 0.8475\n","\n","Epoch 00273: val_loss did not improve from 0.81787\n","Epoch 274/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0985 - acc: 0.9674 - val_loss: 1.1331 - val_acc: 0.8430\n","\n","Epoch 00274: val_loss did not improve from 0.81787\n","Epoch 275/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0991 - acc: 0.9652 - val_loss: 1.0672 - val_acc: 0.8430\n","\n","Epoch 00275: val_loss did not improve from 0.81787\n","Epoch 276/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0731 - acc: 0.9787 - val_loss: 1.1177 - val_acc: 0.8430\n","\n","Epoch 00276: val_loss did not improve from 0.81787\n","Epoch 277/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0896 - acc: 0.9742 - val_loss: 1.0852 - val_acc: 0.8520\n","\n","Epoch 00277: val_loss did not improve from 0.81787\n","Epoch 278/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0970 - acc: 0.9652 - val_loss: 1.1884 - val_acc: 0.8341\n","\n","Epoch 00278: val_loss did not improve from 0.81787\n","Epoch 279/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0999 - acc: 0.9640 - val_loss: 1.1091 - val_acc: 0.8475\n","\n","Epoch 00279: val_loss did not improve from 0.81787\n","Epoch 280/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1100 - acc: 0.9596 - val_loss: 1.2348 - val_acc: 0.8341\n","\n","Epoch 00280: val_loss did not improve from 0.81787\n","Epoch 281/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0751 - acc: 0.9742 - val_loss: 1.1543 - val_acc: 0.8475\n","\n","Epoch 00281: val_loss did not improve from 0.81787\n","Epoch 282/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0775 - acc: 0.9775 - val_loss: 1.1791 - val_acc: 0.8430\n","\n","Epoch 00282: val_loss did not improve from 0.81787\n","Epoch 283/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0812 - acc: 0.9697 - val_loss: 1.2013 - val_acc: 0.8430\n","\n","Epoch 00283: val_loss did not improve from 0.81787\n","Epoch 284/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1002 - acc: 0.9618 - val_loss: 1.1328 - val_acc: 0.8430\n","\n","Epoch 00284: val_loss did not improve from 0.81787\n","Epoch 285/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0939 - acc: 0.9640 - val_loss: 1.1323 - val_acc: 0.8341\n","\n","Epoch 00285: val_loss did not improve from 0.81787\n","Epoch 286/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0824 - acc: 0.9719 - val_loss: 1.2415 - val_acc: 0.8386\n","\n","Epoch 00286: val_loss did not improve from 0.81787\n","Epoch 287/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0699 - acc: 0.9753 - val_loss: 1.1468 - val_acc: 0.8386\n","\n","Epoch 00287: val_loss did not improve from 0.81787\n","Epoch 288/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0944 - acc: 0.9573 - val_loss: 1.2562 - val_acc: 0.8341\n","\n","Epoch 00288: val_loss did not improve from 0.81787\n","Epoch 289/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1640 - acc: 0.9652 - val_loss: 1.1860 - val_acc: 0.8386\n","\n","Epoch 00289: val_loss did not improve from 0.81787\n","Epoch 290/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1045 - acc: 0.9697 - val_loss: 0.9864 - val_acc: 0.8565\n","\n","Epoch 00290: val_loss did not improve from 0.81787\n","Epoch 291/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0822 - acc: 0.9663 - val_loss: 1.1684 - val_acc: 0.8430\n","\n","Epoch 00291: val_loss did not improve from 0.81787\n","Epoch 292/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0930 - acc: 0.9685 - val_loss: 1.0823 - val_acc: 0.8565\n","\n","Epoch 00292: val_loss did not improve from 0.81787\n","Epoch 293/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0790 - acc: 0.9719 - val_loss: 1.1636 - val_acc: 0.8341\n","\n","Epoch 00293: val_loss did not improve from 0.81787\n","Epoch 294/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0595 - acc: 0.9809 - val_loss: 1.2068 - val_acc: 0.8386\n","\n","Epoch 00294: val_loss did not improve from 0.81787\n","Epoch 295/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0726 - acc: 0.9730 - val_loss: 1.1886 - val_acc: 0.8386\n","\n","Epoch 00295: val_loss did not improve from 0.81787\n","Epoch 296/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0909 - acc: 0.9753 - val_loss: 1.6890 - val_acc: 0.7937\n","\n","Epoch 00296: val_loss did not improve from 0.81787\n","Epoch 297/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.2636 - acc: 0.9393 - val_loss: 1.0440 - val_acc: 0.8296\n","\n","Epoch 00297: val_loss did not improve from 0.81787\n","Epoch 298/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1477 - acc: 0.9596 - val_loss: 1.0617 - val_acc: 0.8386\n","\n","Epoch 00298: val_loss did not improve from 0.81787\n","Epoch 299/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1600 - acc: 0.9584 - val_loss: 0.9990 - val_acc: 0.8386\n","\n","Epoch 00299: val_loss did not improve from 0.81787\n","Epoch 300/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4649 - acc: 0.9079 - val_loss: 1.3491 - val_acc: 0.8251\n","\n","Epoch 00300: val_loss did not improve from 0.81787\n","Epoch 301/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.3008 - acc: 0.9213 - val_loss: 0.9393 - val_acc: 0.8520\n","\n","Epoch 00301: val_loss did not improve from 0.81787\n","Epoch 302/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1636 - acc: 0.9539 - val_loss: 1.0194 - val_acc: 0.8386\n","\n","Epoch 00302: val_loss did not improve from 0.81787\n","Epoch 303/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1342 - acc: 0.9618 - val_loss: 1.0705 - val_acc: 0.8475\n","\n","Epoch 00303: val_loss did not improve from 0.81787\n","Epoch 304/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1262 - acc: 0.9708 - val_loss: 1.0517 - val_acc: 0.8475\n","\n","Epoch 00304: val_loss did not improve from 0.81787\n","Epoch 305/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1000 - acc: 0.9730 - val_loss: 1.1328 - val_acc: 0.8475\n","\n","Epoch 00305: val_loss did not improve from 0.81787\n","Epoch 306/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1009 - acc: 0.9719 - val_loss: 1.1605 - val_acc: 0.8430\n","\n","Epoch 00306: val_loss did not improve from 0.81787\n","Epoch 307/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0914 - acc: 0.9708 - val_loss: 1.0791 - val_acc: 0.8520\n","\n","Epoch 00307: val_loss did not improve from 0.81787\n","Epoch 308/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0823 - acc: 0.9764 - val_loss: 1.2618 - val_acc: 0.8430\n","\n","Epoch 00308: val_loss did not improve from 0.81787\n","Epoch 309/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0791 - acc: 0.9708 - val_loss: 1.3224 - val_acc: 0.8386\n","\n","Epoch 00309: val_loss did not improve from 0.81787\n","Epoch 310/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0909 - acc: 0.9663 - val_loss: 1.1437 - val_acc: 0.8520\n","\n","Epoch 00310: val_loss did not improve from 0.81787\n","Epoch 311/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0806 - acc: 0.9697 - val_loss: 1.1258 - val_acc: 0.8475\n","\n","Epoch 00311: val_loss did not improve from 0.81787\n","Epoch 312/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0812 - acc: 0.9708 - val_loss: 1.1315 - val_acc: 0.8430\n","\n","Epoch 00312: val_loss did not improve from 0.81787\n","Epoch 313/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0867 - acc: 0.9663 - val_loss: 1.1395 - val_acc: 0.8565\n","\n","Epoch 00313: val_loss did not improve from 0.81787\n","Epoch 314/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0736 - acc: 0.9742 - val_loss: 1.2411 - val_acc: 0.8520\n","\n","Epoch 00314: val_loss did not improve from 0.81787\n","Epoch 315/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0762 - acc: 0.9730 - val_loss: 1.3249 - val_acc: 0.8475\n","\n","Epoch 00315: val_loss did not improve from 0.81787\n","Epoch 316/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0603 - acc: 0.9843 - val_loss: 1.4107 - val_acc: 0.8296\n","\n","Epoch 00316: val_loss did not improve from 0.81787\n","Epoch 317/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0804 - acc: 0.9663 - val_loss: 1.1547 - val_acc: 0.8430\n","\n","Epoch 00317: val_loss did not improve from 0.81787\n","Epoch 318/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0845 - acc: 0.9708 - val_loss: 1.3398 - val_acc: 0.8251\n","\n","Epoch 00318: val_loss did not improve from 0.81787\n","Epoch 319/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0699 - acc: 0.9708 - val_loss: 1.4702 - val_acc: 0.8206\n","\n","Epoch 00319: val_loss did not improve from 0.81787\n","Epoch 320/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0778 - acc: 0.9697 - val_loss: 1.4374 - val_acc: 0.8341\n","\n","Epoch 00320: val_loss did not improve from 0.81787\n","Epoch 321/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0733 - acc: 0.9719 - val_loss: 1.2829 - val_acc: 0.8565\n","\n","Epoch 00321: val_loss did not improve from 0.81787\n","Epoch 322/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0873 - acc: 0.9629 - val_loss: 1.2627 - val_acc: 0.8475\n","\n","Epoch 00322: val_loss did not improve from 0.81787\n","Epoch 323/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0684 - acc: 0.9809 - val_loss: 1.1580 - val_acc: 0.8520\n","\n","Epoch 00323: val_loss did not improve from 0.81787\n","Epoch 324/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0686 - acc: 0.9764 - val_loss: 1.3684 - val_acc: 0.8341\n","\n","Epoch 00324: val_loss did not improve from 0.81787\n","Epoch 325/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0810 - acc: 0.9663 - val_loss: 1.3409 - val_acc: 0.8430\n","\n","Epoch 00325: val_loss did not improve from 0.81787\n","Epoch 326/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0701 - acc: 0.9730 - val_loss: 1.4553 - val_acc: 0.8386\n","\n","Epoch 00326: val_loss did not improve from 0.81787\n","Epoch 327/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0922 - acc: 0.9607 - val_loss: 1.3115 - val_acc: 0.8565\n","\n","Epoch 00327: val_loss did not improve from 0.81787\n","Epoch 328/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0759 - acc: 0.9742 - val_loss: 1.3026 - val_acc: 0.8206\n","\n","Epoch 00328: val_loss did not improve from 0.81787\n","Epoch 329/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0963 - acc: 0.9663 - val_loss: 1.3154 - val_acc: 0.8430\n","\n","Epoch 00329: val_loss did not improve from 0.81787\n","Epoch 330/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0787 - acc: 0.9674 - val_loss: 1.2312 - val_acc: 0.8610\n","\n","Epoch 00330: val_loss did not improve from 0.81787\n","Epoch 331/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0757 - acc: 0.9742 - val_loss: 1.2919 - val_acc: 0.8520\n","\n","Epoch 00331: val_loss did not improve from 0.81787\n","Epoch 332/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0666 - acc: 0.9798 - val_loss: 1.4497 - val_acc: 0.8296\n","\n","Epoch 00332: val_loss did not improve from 0.81787\n","Epoch 333/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0794 - acc: 0.9685 - val_loss: 1.3142 - val_acc: 0.8386\n","\n","Epoch 00333: val_loss did not improve from 0.81787\n","Epoch 334/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0716 - acc: 0.9742 - val_loss: 1.3827 - val_acc: 0.8386\n","\n","Epoch 00334: val_loss did not improve from 0.81787\n","Epoch 335/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0846 - acc: 0.9674 - val_loss: 1.4183 - val_acc: 0.8430\n","\n","Epoch 00335: val_loss did not improve from 0.81787\n","Epoch 336/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0726 - acc: 0.9730 - val_loss: 1.3695 - val_acc: 0.8475\n","\n","Epoch 00336: val_loss did not improve from 0.81787\n","Epoch 337/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0717 - acc: 0.9764 - val_loss: 1.4593 - val_acc: 0.8341\n","\n","Epoch 00337: val_loss did not improve from 0.81787\n","Epoch 338/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0783 - acc: 0.9685 - val_loss: 1.3124 - val_acc: 0.8610\n","\n","Epoch 00338: val_loss did not improve from 0.81787\n","Epoch 339/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0842 - acc: 0.9719 - val_loss: 1.3467 - val_acc: 0.8296\n","\n","Epoch 00339: val_loss did not improve from 0.81787\n","Epoch 340/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0687 - acc: 0.9787 - val_loss: 1.3092 - val_acc: 0.8565\n","\n","Epoch 00340: val_loss did not improve from 0.81787\n","Epoch 341/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0577 - acc: 0.9798 - val_loss: 1.5184 - val_acc: 0.8341\n","\n","Epoch 00341: val_loss did not improve from 0.81787\n","Epoch 342/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1096 - acc: 0.9607 - val_loss: 1.4528 - val_acc: 0.8251\n","\n","Epoch 00342: val_loss did not improve from 0.81787\n","Epoch 343/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0893 - acc: 0.9607 - val_loss: 1.0947 - val_acc: 0.8520\n","\n","Epoch 00343: val_loss did not improve from 0.81787\n","Epoch 344/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0941 - acc: 0.9674 - val_loss: 1.1772 - val_acc: 0.8475\n","\n","Epoch 00344: val_loss did not improve from 0.81787\n","Epoch 345/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0639 - acc: 0.9787 - val_loss: 1.2104 - val_acc: 0.8475\n","\n","Epoch 00345: val_loss did not improve from 0.81787\n","Epoch 346/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0666 - acc: 0.9787 - val_loss: 1.1841 - val_acc: 0.8475\n","\n","Epoch 00346: val_loss did not improve from 0.81787\n","Epoch 347/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0685 - acc: 0.9798 - val_loss: 1.1459 - val_acc: 0.8520\n","\n","Epoch 00347: val_loss did not improve from 0.81787\n","Epoch 348/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0856 - acc: 0.9685 - val_loss: 1.4374 - val_acc: 0.8296\n","\n","Epoch 00348: val_loss did not improve from 0.81787\n","Epoch 349/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0508 - acc: 0.9843 - val_loss: 1.3909 - val_acc: 0.8430\n","\n","Epoch 00349: val_loss did not improve from 0.81787\n","Epoch 350/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0720 - acc: 0.9742 - val_loss: 1.2368 - val_acc: 0.8520\n","\n","Epoch 00350: val_loss did not improve from 0.81787\n","Epoch 351/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0786 - acc: 0.9708 - val_loss: 1.3281 - val_acc: 0.8475\n","\n","Epoch 00351: val_loss did not improve from 0.81787\n","Epoch 352/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0998 - acc: 0.9764 - val_loss: 1.4436 - val_acc: 0.8386\n","\n","Epoch 00352: val_loss did not improve from 0.81787\n","Epoch 353/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.4627 - acc: 0.9135 - val_loss: 1.5716 - val_acc: 0.7803\n","\n","Epoch 00353: val_loss did not improve from 0.81787\n","Epoch 354/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.4112 - acc: 0.9056 - val_loss: 1.3795 - val_acc: 0.7937\n","\n","Epoch 00354: val_loss did not improve from 0.81787\n","Epoch 355/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.3514 - acc: 0.9124 - val_loss: 1.1662 - val_acc: 0.8161\n","\n","Epoch 00355: val_loss did not improve from 0.81787\n","Epoch 356/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1840 - acc: 0.9449 - val_loss: 1.2106 - val_acc: 0.8251\n","\n","Epoch 00356: val_loss did not improve from 0.81787\n","Epoch 357/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1157 - acc: 0.9719 - val_loss: 1.1213 - val_acc: 0.8296\n","\n","Epoch 00357: val_loss did not improve from 0.81787\n","Epoch 358/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.1032 - acc: 0.9652 - val_loss: 1.1272 - val_acc: 0.8386\n","\n","Epoch 00358: val_loss did not improve from 0.81787\n","Epoch 359/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0968 - acc: 0.9685 - val_loss: 1.1152 - val_acc: 0.8296\n","\n","Epoch 00359: val_loss did not improve from 0.81787\n","Epoch 360/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0896 - acc: 0.9775 - val_loss: 1.3330 - val_acc: 0.8296\n","\n","Epoch 00360: val_loss did not improve from 0.81787\n","Epoch 361/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0993 - acc: 0.9652 - val_loss: 1.2242 - val_acc: 0.8296\n","\n","Epoch 00361: val_loss did not improve from 0.81787\n","Epoch 362/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0753 - acc: 0.9753 - val_loss: 1.3349 - val_acc: 0.8206\n","\n","Epoch 00362: val_loss did not improve from 0.81787\n","Epoch 363/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0824 - acc: 0.9697 - val_loss: 1.2595 - val_acc: 0.8430\n","\n","Epoch 00363: val_loss did not improve from 0.81787\n","Epoch 364/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0668 - acc: 0.9798 - val_loss: 1.3214 - val_acc: 0.8251\n","\n","Epoch 00364: val_loss did not improve from 0.81787\n","Epoch 365/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0548 - acc: 0.9820 - val_loss: 1.3701 - val_acc: 0.8386\n","\n","Epoch 00365: val_loss did not improve from 0.81787\n","Epoch 366/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0785 - acc: 0.9753 - val_loss: 1.3489 - val_acc: 0.8206\n","\n","Epoch 00366: val_loss did not improve from 0.81787\n","Epoch 367/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0672 - acc: 0.9798 - val_loss: 1.2985 - val_acc: 0.8475\n","\n","Epoch 00367: val_loss did not improve from 0.81787\n","Epoch 368/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0856 - acc: 0.9742 - val_loss: 1.4035 - val_acc: 0.8386\n","\n","Epoch 00368: val_loss did not improve from 0.81787\n","Epoch 369/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1646 - acc: 0.9483 - val_loss: 1.3421 - val_acc: 0.8161\n","\n","Epoch 00369: val_loss did not improve from 0.81787\n","Epoch 370/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1359 - acc: 0.9596 - val_loss: 1.1684 - val_acc: 0.8161\n","\n","Epoch 00370: val_loss did not improve from 0.81787\n","Epoch 371/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1585 - acc: 0.9427 - val_loss: 1.0691 - val_acc: 0.8341\n","\n","Epoch 00371: val_loss did not improve from 0.81787\n","Epoch 372/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.1144 - acc: 0.9584 - val_loss: 1.0682 - val_acc: 0.8251\n","\n","Epoch 00372: val_loss did not improve from 0.81787\n","Epoch 373/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0897 - acc: 0.9708 - val_loss: 1.1057 - val_acc: 0.8475\n","\n","Epoch 00373: val_loss did not improve from 0.81787\n","Epoch 374/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0768 - acc: 0.9742 - val_loss: 1.1346 - val_acc: 0.8430\n","\n","Epoch 00374: val_loss did not improve from 0.81787\n","Epoch 375/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0770 - acc: 0.9753 - val_loss: 1.2213 - val_acc: 0.8475\n","\n","Epoch 00375: val_loss did not improve from 0.81787\n","Epoch 376/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0651 - acc: 0.9787 - val_loss: 1.2437 - val_acc: 0.8520\n","\n","Epoch 00376: val_loss did not improve from 0.81787\n","Epoch 377/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0833 - acc: 0.9685 - val_loss: 1.1984 - val_acc: 0.8475\n","\n","Epoch 00377: val_loss did not improve from 0.81787\n","Epoch 378/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0632 - acc: 0.9775 - val_loss: 1.2913 - val_acc: 0.8520\n","\n","Epoch 00378: val_loss did not improve from 0.81787\n","Epoch 379/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0748 - acc: 0.9753 - val_loss: 1.3305 - val_acc: 0.8655\n","\n","Epoch 00379: val_loss did not improve from 0.81787\n","Epoch 380/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0880 - acc: 0.9674 - val_loss: 1.2724 - val_acc: 0.8610\n","\n","Epoch 00380: val_loss did not improve from 0.81787\n","Epoch 381/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0558 - acc: 0.9809 - val_loss: 1.3087 - val_acc: 0.8475\n","\n","Epoch 00381: val_loss did not improve from 0.81787\n","Epoch 382/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0620 - acc: 0.9787 - val_loss: 1.3231 - val_acc: 0.8520\n","\n","Epoch 00382: val_loss did not improve from 0.81787\n","Epoch 383/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0612 - acc: 0.9787 - val_loss: 1.2912 - val_acc: 0.8475\n","\n","Epoch 00383: val_loss did not improve from 0.81787\n","Epoch 384/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0486 - acc: 0.9876 - val_loss: 1.4807 - val_acc: 0.8520\n","\n","Epoch 00384: val_loss did not improve from 0.81787\n","Epoch 385/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0860 - acc: 0.9719 - val_loss: 1.4923 - val_acc: 0.8430\n","\n","Epoch 00385: val_loss did not improve from 0.81787\n","Epoch 386/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0668 - acc: 0.9787 - val_loss: 1.5859 - val_acc: 0.8386\n","\n","Epoch 00386: val_loss did not improve from 0.81787\n","Epoch 387/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0453 - acc: 0.9865 - val_loss: 1.4935 - val_acc: 0.8520\n","\n","Epoch 00387: val_loss did not improve from 0.81787\n","Epoch 388/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0640 - acc: 0.9809 - val_loss: 1.5445 - val_acc: 0.8341\n","\n","Epoch 00388: val_loss did not improve from 0.81787\n","Epoch 389/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0740 - acc: 0.9697 - val_loss: 1.3622 - val_acc: 0.8430\n","\n","Epoch 00389: val_loss did not improve from 0.81787\n","Epoch 390/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0543 - acc: 0.9854 - val_loss: 1.4632 - val_acc: 0.8475\n","\n","Epoch 00390: val_loss did not improve from 0.81787\n","Epoch 391/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0603 - acc: 0.9865 - val_loss: 1.5135 - val_acc: 0.8430\n","\n","Epoch 00391: val_loss did not improve from 0.81787\n","Epoch 392/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0709 - acc: 0.9764 - val_loss: 1.4976 - val_acc: 0.8475\n","\n","Epoch 00392: val_loss did not improve from 0.81787\n","Epoch 393/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0592 - acc: 0.9753 - val_loss: 1.4919 - val_acc: 0.8520\n","\n","Epoch 00393: val_loss did not improve from 0.81787\n","Epoch 394/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0540 - acc: 0.9809 - val_loss: 1.5073 - val_acc: 0.8430\n","\n","Epoch 00394: val_loss did not improve from 0.81787\n","Epoch 395/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0510 - acc: 0.9787 - val_loss: 1.5360 - val_acc: 0.8386\n","\n","Epoch 00395: val_loss did not improve from 0.81787\n","Epoch 396/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0832 - acc: 0.9685 - val_loss: 1.5524 - val_acc: 0.8296\n","\n","Epoch 00396: val_loss did not improve from 0.81787\n","Epoch 397/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0839 - acc: 0.9685 - val_loss: 1.5007 - val_acc: 0.8430\n","\n","Epoch 00397: val_loss did not improve from 0.81787\n","Epoch 398/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0588 - acc: 0.9775 - val_loss: 1.5266 - val_acc: 0.8430\n","\n","Epoch 00398: val_loss did not improve from 0.81787\n","Epoch 399/400\n","890/890 [==============================] - 4s 4ms/step - loss: 0.0677 - acc: 0.9809 - val_loss: 1.4389 - val_acc: 0.8475\n","\n","Epoch 00399: val_loss did not improve from 0.81787\n","Epoch 400/400\n","890/890 [==============================] - 3s 4ms/step - loss: 0.0703 - acc: 0.9730 - val_loss: 1.4806 - val_acc: 0.8386\n","\n","Epoch 00400: val_loss did not improve from 0.81787\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I09HH9u6RnBB","colab_type":"code","colab":{}},"source":[" model = load_model(\"model.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ywXQjbwRnEl","colab_type":"code","colab":{}},"source":["def predictions(text):\n","  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n","  test_word = word_tokenize(clean)\n","  test_word = [w.lower() for w in test_word]\n","  test_ls = word_tokenizer.texts_to_sequences(test_word)\n","  print(test_word)\n","  #Check for unknown words\n","  if [] in test_ls:\n","    test_ls = list(filter(None, test_ls))\n","    \n","  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n"," \n","  x = padding_doc(test_ls, max_length)\n","  \n","  pred = model.predict_proba(x)\n","  \n","  \n","  return pred\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b2FmntxRnIH","colab_type":"code","colab":{}},"source":["def get_final_output(pred, classes):\n","  predictions = pred[0]\n"," \n","  classes = np.array(classes)\n","  ids = np.argsort(-predictions)\n","  classes = classes[ids]\n","  predictions = -np.sort(-predictions)\n"," \n","  for i in range(pred.shape[1]):\n","    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bysvczHfRnLm","colab_type":"code","outputId":"85687284-1b26-47d4-e46d-ce2ce098f445","executionInfo":{"status":"ok","timestamp":1567503077296,"user_tz":-330,"elapsed":998,"user":{"displayName":"Saurabh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBttMcU30JThjsbyn5xGJw0WSLLoV0shNIyS2jZ=s64","userId":"11443495427741628084"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["text = \"How are you?\"\n","pred = predictions(text)\n","get_final_output(pred, unique_intent)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["['how', 'are', 'you']\n","faq.apply_register has confidence = 0.26950818\n","commonQ.bot has confidence = 0.19185019\n","commonQ.assist has confidence = 0.17722239\n","commonQ.name has confidence = 0.12423032\n","commonQ.how has confidence = 0.112197556\n","commonQ.query has confidence = 0.08505256\n","contact.contact has confidence = 0.01820928\n","faq.bad_service has confidence = 0.014063595\n","faq.aadhaar_missing has confidence = 0.0021513915\n","faq.application_process has confidence = 0.0016861191\n","commonQ.wait has confidence = 0.0014568105\n","commonQ.not_giving has confidence = 0.0009298332\n","faq.approval_time has confidence = 0.0004958567\n","faq.borrow_limit has confidence = 0.0004185762\n","faq.biz_new has confidence = 0.00019259713\n","faq.biz_simpler has confidence = 0.00017081565\n","faq.borrow_use has confidence = 9.644657e-05\n","faq.address_proof has confidence = 2.471375e-05\n","faq.banking_option_missing has confidence = 1.5562335e-05\n","faq.biz_category_missing has confidence = 1.5546017e-05\n","commonQ.just_details has confidence = 1.1653938e-05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FOCW13Lgdsr7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}